# DisasterResponseProject


### Overview
In the wake of a disaster, sorting through the flood of social media messages from an affected area and allocating resources and aid to those who need it can be challenging.  

This project uses an ETL pipeline, ML model, and a web application learn from real tweets and classify novel ones in an attempt to offer a  solution to this problem.  

There are four components in this repository:
   1) Raw data and category labels
   2) ETL pipline to organize and clean the data
   3) An ML model to categorize tweets
   4) A web-based application to categorize novel text 

### Files
This project has three folders: data, models, and app

The data folder contains:
   * process_data.py: this loads data from the csv's, joins the data, and stores it in an sqlite database
   * disaster_messages.csv: a file containing the messages (tweets) 
   * disaster_categories: a file containing the categories into which the messages are classified
   * DiasterResponse.db: a database with data that can be regenerated by running process_data.py

The models folder contains:
   * train_classifier.py: a script that trains an ML model by loading data in DiasterResponse.db that was generated using the       

The app folder contains: 
   * run.py: a script to launch the web app and show data visualizations

### Instructions

1. In a terminal, run the script to clean the data and save to the database:
      `python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db`
2. Next, run the script to train and save a classifying model:
      `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`
3. To launch the web app:
      `python run.py`
4. Type 
      'env|grep WORK'
5. Use the URL https://SPACEID-3001.SPACEDOMAIN with the SPACEID and SPACEDOMAIN from step 4 
